# -*- coding: utf-8 -*-
"""Data Cleaning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IGmdQ9FWVPf_lo0hZMwBW-QPHpHJ4aAO
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy
# %matplotlib inline
import io

# import my data 
from google.colab import files
data_to_load = files.upload()

# Users may need to use utf-8 for encoding but ISO-8859-1 worked for me 
candy_df = pd.read_csv(io.BytesIO(data_to_load['candy.csv']), encoding = "ISO-8859-1")
print(candy_df.head())

"""The dataset includes the following variables:

* **Internal ID:** A unique indentifier for every record in the database. 
* **Going Out?:** Binary field (Yes/No).
* **Gender:** Three different options (Male, Female, I'd rather not say).
* **Age:** Numerical field.
* **Country:** This is a text field but looks like people have entered their own versions --> USA instead of America.
* **State/Province:** Text field, again with users entering own versions. 
* **Joy Or Despair:** Three different options to rate candy (Joy, Meh, Despair).
* **Joy Other:** Text field.
* **Despair Other:** Text field.
* **Other Comments:** Text field.
* **Dress:** Text field.
* **Unnamed:**
* **Day:** Binary field.
* **Media:** Images/click fields. 

The goal of this dataset is for data cleaning. Starting off I will remove the column named "Unnamed: 113" as I do not know what the values are for (even though there are plenty of values in that column). Since the goal is for datacleaning, all other columns will be kept, even though in other scenarios when building models some may be dropped. 
"""

# drop the columns we will not be working with
candy_df.drop(["Unnamed: 113"], axis=1, inplace=True)

"""Changing the column names to be all lower case to make the data easier to work with. If you would like you can also change the names of the columns as below, 

candy_df = candy_df.rename(columns = {'Q1: GOING OUT?' : 'going_out?', 'Q2: GENDER' : 'gender'}) and so on. 
"""

candy_df.columns = candy_df.columns.str.lower()

"""Lets take a look at missing values with percentage. """

def missing_data(df):
  df = df.isnull().sum()*100/df.isnull().count()
  print(df)
missing_data(candy_df)

"""By looking at the above we can tell that Media [Yahoo], Media[ESPN], Media[Daily Dish], Despair Other, and Other Comments are columns with more than 70% of missing values. I will go ahead and drop these columns by using the same drop method above."""

# drop the columns we will not be working with
candy_df.drop(['q8: despair other','q9: other comments','q12: media [yahoo]', 
               'q12: media [daily dish]','q12: media [espn]', 'click coordinates (x, y)'], axis=1, inplace=True)

"""Now lets look at other fields and figure out how to handle all other missing values. """

print(candy_df['q2: gender'].isna().value_counts())

"""The above tells us that there are 41 missing values with the Gender column. This field is not just "Male" or "Female, it also includes "I'd rather not say" and "Other". Because the field is left open, the person filling out the form may have not wanted to give this information so I will fill the values with "I'd rather not say". """

candy_df['q2: gender'].fillna(value="I'd rather not say", inplace=True)

"""The country column is not a pretty one. The form is completed by writing in a response so the answers vary. To make sure I do not mess up the dataframe when figuring out the best way to move forward I will first make a copy of the dataframe.

When using a form and data manually being entered it can lead to errors. These errors could be misspelling, entering mis-information, capital letters where they should'nt be, and white spaces. Lets start with fixing white spaces and capitalization and see if that does any good.
"""

candy_df['q4: country'] = candy_df['q4: country'].str.lower()
candy_df['q4: country'] = candy_df['q4: country'].str.strip()
missing_data(candy_df)

"""Stripping the data of white spaces did nothing. Let's use unique() which will list all unique values in order of appererance. This will give us a good representation of what we're dealing with. """

candy_df["q4: country"].unique()

"""There are a lot of different ways to deal with this data. Using regex to find patterns or replace words. We could use fuzzywuzzy a python method to match strings looks at differences in sequences. I'll be using replace which you can see below. """

candy_df["q4: country"] = candy_df["q4: country"].replace(['usa', 'us', 
        'murica', 'united states', 'united staes', 'united states of america',
       'uae', 'u.s.a.', 'usausausa', 'america', '35', 'unhinged states', 'us of a', 'unites states',
       'the united states', 'north carolina', 'unied states', 'earth', 'u s', 'u.s.', 
       'costa rica', 'the united states of america', 'unite states', '46', 'usa? hard to tell anymore..', "'merica", 'usas', 'pittsburgh',
       '45', 'united state', '32', 'a', 'new york', 'united sates', 'california', 
       'i pretend to be from canada, but i am really from the united states.', 
       'united stated', 'ahem....amerca', 'new jersey', 'united ststes','united statss',
       'atlantis', 'murrika', 'usa! usa! usa!', 'usaa',
       'alaska', 'n. america', 'ussa', 'narnia',
       'u s a', 'united statea', '1', 'subscribe to dm4uz3 on youtube',
       'usa usa usa!!!!', "i don't know anymore", 'fear and loathing', 'insanity lately'], "usa")

candy_df["q4: country"].unique()

"""Seemed to clean up a lot of the country column. Now that we have what we want I'll use mode to replace any missing data. You could also replace missing data with "Do not know" or anything that makes sense. """

candy_df["q4: country"].fillna(candy_df["q4: country"].mode()[0], inplace=True)

# check to see the missing data in country column
missing_data(candy_df)

"""Moving on to age column I'll be using the code below to check the different problems that occur within the column. """

# Print all values that cannot be converted to float
for column_name in ["q3: age"]:
  print("These are the problematic values for the variable: {}".format(column_name))
  lst = []
  for value in candy_df[column_name]:
    try:
      float(value)
    except:
      print(value)
      lst.append(value)

"""People are creative! Again I'll be replacing any entry that doesnt make sense with np.nan and then we need to downcast to a float and use the mean to fill. """

candy_df = candy_df.replace(["Old enough", "?", "Many", "no", "45-55", "hahahahaha",
                             "older than dirt", "5u", "Enough", "See question 2",
                             "24-50", "Over 50", "sixty-nine", "46 Halloweens.",
                             "ancient", "OLD", "old", "70 1/2", "MY NAME JEFF",
                             "59 on the day after Halloween", "old enough", "your mom",
                             "I can remember when Java was a cool new language", "60+"], np.nan)

# Print all values that cannot be converted to float
for column_name in ["q3: age"]:
  print("These are the problematic values for the variable: {}".format(column_name))
  lst = []
  for value in candy_df[column_name]:
    try:
      float(value)
    except:
      print(value)
      lst.append(value)

candy_df["q3: age"] = pd.to_numeric(candy_df["q3: age"], downcast="float")
candy_df["q3: age"].fillna(candy_df["q3: age"].mean(), inplace=True)

# check age column for missing data 
missing_data(candy_df)

"""Going out column has some missing data as well. This entry should only accept Yes/No but for any missing columns here I will change to "Not sure" """

candy_df["q1: going out?"] = candy_df["q1: going out?"].fillna("Not sure")

"""If we have a closer look at the dataframe, we have a lot of missing data still. With an even closer look we can see that a lot of people filling out this survey left all answers empty when rating candy (which should accept three entries, Joy, Meh, Despair). Depending on what we're using the data for, that might be the primary focus. Due to this, below I will be dropping any row that does not have more than two candy rating questions filled out on their survey. """

# make a list of all column names 
my_list = candy_df.columns.values.tolist()

# initiate a new list and slice on the columns we need (candy ratings)
cols = my_list[6:-3]

len(cols)

"""At the moment, after all the cleaning we've done we're left with 2460 rows and 113 columns. Lets see what happens after we drop rows that do not have more than two candy ratings filled out. """

candy_df.shape

"""We use cols as our column list and set the thresh at 2. """

candy_df = candy_df.dropna(subset=cols, thresh=2)

candy_df.shape

"""We dropped a significant amount of rows and our missing data improved greatly. """

missing_data(candy_df)

"""This is a great start and will continue to add code and update as time allows. """